{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ames Housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the value of SalePrice variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_dataset(data_dir, train_fn, test_fn):\n",
    "\n",
    "    train_path = os.path.join(data_dir, train_fn)\n",
    "    test_path = os.path.join(data_dir, test_fn)\n",
    "\n",
    "    return pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "\n",
    "train_set, test_set = get_dataset('dataset', 'train.csv', 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store Id column and drop them from training and test set (not necessary for predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ID = train_set['Id']\n",
    "test_ID = test_set['Id']\n",
    "\n",
    "X_train = train_set.drop('Id', axis=1)\n",
    "X_test = test_set.drop('Id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render **histogram of dependent variable** `SalePrice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density_hist(x):\n",
    "    sns.histplot(x, stat='density', kde=False, bins=50)\n",
    "    sns.kdeplot(x, cut=3, color='crimson')\n",
    "\n",
    "plot_density_hist(X_train['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deviate from normal distribution\n",
    "- Positive skewed\n",
    "- Show peakedness (high *kurtosis*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Skewness: {X_train.SalePrice.skew():.2f}')\n",
    "print(f'Kurtosis: {X_train.SalePrice.kurt():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how variables are correlated using a **Correlation Matrix (heatmap)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = X_train.corr()\n",
    "f, ax = plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice bright white squares. These are strong linear correlations between nearing variables. The correlation is so strong that it could indicate that multicollinearity. The biggest perpetrators are `TotalBsmtSF` with `1stFlrSF`, `GarageCars` with `GarageArea`, `GrLivArea` with `TotRmsAbvGrd`, and `YearBuilt` with `GarageYrBlt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that the strong correlations between certain variables are our dependent variable, SalePrice. This includes `OverallQual`, `GrLivArea`, `GarageCars`, `GarageArea`, `TotalBsmtSF`, `1stFlrSF`, and to some extent `YearBuilt`, `FullBath`, and a few others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zoomed correlation matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 # number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(X_train[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see which variables have the strongest linear correlation to `SalePrice`. From the previous heatmap we know that both pairs `(GarageCars, GarageArea)` and `(TotalBsmtSF, 1stFlrSF)` are strongly related. Therefore, we can ignore one from each pair to avoid multicollinearity. Let's keep `GarageCars` and `TotalBsmtSF` since they have a slightly stronger correlation to `SalePrice`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that `GrLivArea` and `TotRmsAbvGrd` have a strong correlation, suggesting mulitcollinearity. Perhaps we can drop `TotalRmsAbvGrd` as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scatter plots between `SalePrice` and correlated variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "sns.pairplot(X_train[cols], size=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation suggests there may be outliers present in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = X_train['GrLivArea'], y = X_train['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove two values in the bottom right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting outliers \n",
    "X_train = X_train.drop(X_train[(X_train['GrLivArea']>4000) & (X_train['SalePrice']<300000)].index)\n",
    "\n",
    "#Check the graphic again\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_train['GrLivArea'], X_train['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful deleting too many outliers because they may show up in the test set. We want the models to be robust enough to handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SalePrice` is the dependent variable (what we need to predict) so let's do some analysis on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(mean, std, color=\"black\"):\n",
    "    x = np.linspace(mean-4*std, mean+4*std)\n",
    "    p = stats.norm.pdf(x, mean, std)\n",
    "    z = plt.plot(x, p, color, linewidth=2)\n",
    "\n",
    "plot_density_hist(X_train['SalePrice'])\n",
    "\n",
    "# get fitted parameters used by the function\n",
    "(mu, sigma) = stats.norm.fit(X_train['SalePrice'])\n",
    "print(f'\\n mu = {mu:.2f} and sigma = {sigma:.2f}\\n')\n",
    "\n",
    "normal(mu, sigma)\n",
    "\n",
    "# now plot the distribution\n",
    "plt.legend([f'Normal dist. ($\\mu=$ {mu:.2f} and $\\sigma=$ {sigma:.2f}'], loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "# also get the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(X_train['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target variable is right skewed. Linear models love normally distributed data, so we need to transform this variable to make it more normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Log-Transformation of the target variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use np.log1p which applies log(1+x) to all elements of the column\n",
    "X_train['SalePrice'] = np.log1p(X_train['SalePrice'])\n",
    "\n",
    "plot_density_hist(X_train['SalePrice'])\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = stats.norm.fit(X_train['SalePrice'])\n",
    "print(f'\\n mu = {mu:.2f} and sigma = {sigma:.2f}\\n')\n",
    "\n",
    "# normal(mu, sigma)\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend([f'Normal dist. ($\\mu=$ {mu:.2f} and $\\sigma=$ {sigma:.2f} )'],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(X_train['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Features engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unecessary and problem (multicollinearity) features from training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train['SalePrice']\n",
    "X_train = X_train.drop(['SalePrice', '1stFlrSF', 'GarageArea', 'TotRmsAbvGrd', 'GarageYrBlt'], axis=1)\n",
    "X_test = X_test.drop(['1stFlrSF', 'GarageArea', 'TotRmsAbvGrd', 'GarageYrBlt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Missing Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important questions when thinking about missing data:\n",
    "- How prevalent is the missing data?\n",
    "- Is missing data random or does it have a pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([X_train, X_test]).reset_index(drop=True)\n",
    "total = all_data.isnull().sum().sort_values(ascending=False)\n",
    "percent = (all_data.isnull().sum()/all_data.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are elements missing from both training and test sets. We'll have to fill them in if possible. Note that we don't want to introduce bias to the test set by filling them in with any value. If the documentation states that a missing value represents something (like None or 0) then we will fill both sets with those values. Otherwise, we'll have to find the most common or median value within the training set and fill the test set with those values. If values are missing from a feature in the test set but no in the training set, we are left with no other option besides filling it in with whatever we can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptions for the following variables states that NA means:\n",
    "- `PoolQC`: no pool\n",
    "- `MiscFeatures`: no misc features\n",
    "- `Alley`: no alley\n",
    "- `Fence`: no fence\n",
    "- `FireplaceQu`: no fireplace\n",
    "- `GarageType`, `GarageFinish`, `GarageQual`, `GarageCond`: no garage\n",
    "- `BsmtQual`, `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`: no basement\n",
    "- `MasVnrType`: no masonry veneer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', \n",
    "    'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType',\n",
    "    'MSSubClass']:\n",
    "    X_train[col] = X_train[col].fillna('None')\n",
    "    X_test[col] = X_test[col].fillna('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `GarageYrBlt`, `GarageArea`, `GarageCars`: replace missing data with 0 (since No garage means no cars in such garage)\n",
    "- `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnSF`, `TotalBsmtSF`, `BsmtFullBath`, `BsmtHalfBath`: missing values are likely zero for having no basement\n",
    "- `MasVnrArea`: likely 0 for no masonry veneer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', \n",
    "    'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']:\n",
    "    X_train[col] = X_train[col].fillna(0)\n",
    "    X_test[col] = X_test[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing values with most frequent category for:\n",
    "- `MSZoning`\n",
    "- `Electrical`\n",
    "- `KitchenQual`\n",
    "- `Exterior1st`\n",
    "- `Exterior2nd`\n",
    "- `SaleType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'MSZoning']:\n",
    "    X_train[col] = X_train[col].fillna(X_train[col].mode()[0])\n",
    "    X_test[col] = X_test[col].fillna(X_test[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Utilities`: This feature won't help in predictive modeling so we can safely remove it (every instance in the training set is in the same category). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('Utilities', axis=1)\n",
    "X_test = X_test.drop('Utilities', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Function` feature with value NaN refers to \"Typical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"Functional\"] = X_train[\"Functional\"].fillna(\"Typ\")\n",
    "X_test[\"Functional\"] = X_test[\"Functional\"].fillna(\"Typ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LotFrontage`: since lots in the same neighborhood are likely to have similar sizes, fill with median LotFrontage of neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find median LotFrontage for a neighborhood\n",
    "lot_med = X_train.groupby('Neighborhood')['LotFrontage'].median()\n",
    "\n",
    "# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n",
    "X_train['LotFrontage'] = X_train.groupby('Neighborhood')['LotFrontage'].transform(\n",
    "    lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Fill missing values in X_test with medians from each neighborhood in X\n",
    "for i, nan in enumerate(X_test['LotFrontage'].isnull()):\n",
    "    if nan:\n",
    "        neighborhood = X_test.loc[i, 'Neighborhood']\n",
    "        X_test.loc[i, 'LotFrontage'] = lot_med[neighborhood]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any remaining missing values in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check remaining missing values if any \n",
    "all_data_na = ((X_train.isnull().sum() + X_test.isnull().sum()) / (len(X_train) + len(X_test))) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transforming some numerical variables that are really categorical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSSubClass=The building class\n",
    "X_train['MSSubClass'] = X_train['MSSubClass'].apply(str)\n",
    "\n",
    "#Changing OverallCond into a categorical variable\n",
    "X_train['OverallCond'] = X_train['OverallCond'].astype(str)\n",
    "\n",
    "#Year and month sold are transformed into categorical features.\n",
    "X_train['YrSold'] = X_train['YrSold'].astype(str)\n",
    "X_train['MoSold'] = X_train['MoSold'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSSubClass=The building class\n",
    "X_test['MSSubClass'] = X_test['MSSubClass'].apply(str)\n",
    "\n",
    "#Changing OverallCond into a categorical variable\n",
    "X_test['OverallCond'] = X_test['OverallCond'].astype(str)\n",
    "\n",
    "#Year and month sold are transformed into categorical features.\n",
    "X_test['YrSold'] = X_test['YrSold'].astype(str)\n",
    "X_test['MoSold'] = X_test['MoSold'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label Encoding some categorical variables that may contain information in their ordering set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode(X):\n",
    "    cols = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "            'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "            'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "            'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "            'YrSold', 'MoSold']\n",
    "    # process columns, apply LabelEncoder to categorical features\n",
    "    for c in cols:\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(X[c].values))\n",
    "        X[c] = lbl.transform(list(X[c].values))\n",
    "\n",
    "    print(X.shape)\n",
    "\n",
    "label_encode(X_train)\n",
    "label_encode(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skewed Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = X_train.dtypes[X_train.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = X_train[numeric_feats].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some heavily skewed features in the training set. They probably exist in the test set as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Box Cox Transformation of (highly) skewed features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the scipy function boxcox1p which computes the Box-Cox transformation of $1+x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that setting $\\lambda=0$ is equivalent to log1p used above for the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_skewness(X):\n",
    "    numeric_feats = X.dtypes[X.dtypes != \"object\"].index\n",
    "\n",
    "    # Check the skew of all numerical features\n",
    "    skewed_feats = X[numeric_feats].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)\n",
    "    skewness_df = pd.DataFrame({'Skew' :skewed_feats})\n",
    "\n",
    "    skewness = skewness_df[abs(skewness_df) > 0.75]\n",
    "    print(f\"Correcting {skewness.shape[0]} numerical features to Box Cox transform\")\n",
    "\n",
    "    from scipy.special import boxcox1p\n",
    "    skewed_features = skewness.index\n",
    "    lam = 0.15\n",
    "    for f in skewed_features:\n",
    "        #X[f] += 1\n",
    "        X[f] = boxcox1p(X[f], lam)\n",
    "        \n",
    "    #X[skewed_features] = np.log1p(X[skewed_features])\n",
    "\n",
    "correct_skewness(X_train)\n",
    "correct_skewness(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encoding features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_attr = X_train.select_dtypes(include='object').columns\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "ohe_cols_train = pd.DataFrame(ohe.fit_transform(X_train[cat_attr]))\n",
    "ohe_cols_test = pd.DataFrame(ohe.transform(X_test[cat_attr]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "ohe_cols_train.index = X_train.index\n",
    "ohe_cols_test.index = X_test.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "numeric_X_train = X_train.drop(cat_attr, axis=1)\n",
    "numeric_X_test = X_test.drop(cat_attr, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "X_train = pd.concat([numeric_X_train, ohe_cols_train], axis=1)\n",
    "X_test = pd.concat([numeric_X_test, ohe_cols_test], axis=1)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "k, kfold = 5, StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = kfold.get_n_splits(X_train.values)\n",
    "    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=kf))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('model', Lasso(random_state=42))\n",
    "])\n",
    "\n",
    "lasso.fit(X_train, y_train)\n",
    "score = rmsle_cv(lasso)\n",
    "print(f'Lasso: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ElasticNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "en = Pipeline([\n",
    "    ('scaler', RobustScaler()), \n",
    "    ('model', ElasticNet(random_state=42))\n",
    "])\n",
    "\n",
    "en.fit(X_train, y_train)\n",
    "score = rmsle_cv(en)\n",
    "print(f'ElasticNet: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel Ridge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "kr = KernelRidge()\n",
    "kr.fit(X_train, y_train)\n",
    "score = rmsle_cv(kr)\n",
    "print(f'Kernel Ridge: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor(random_state=42, nthread=-1)\n",
    "xgb.fit(X_train, y_train)\n",
    "score = rmsle_cv(xgb)\n",
    "print(f'XGBoost: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# huber loss makes it robust to outliers\n",
    "gb = GradientBoostingRegressor(loss='huber', random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "score = rmsle_cv(gb)\n",
    "print(f'Gradient Boosting: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgb = LGBMRegressor()\n",
    "lgb.fit(X_train, y_train)\n",
    "score = rmsle_cv(lgb)\n",
    "print(f'LightGB: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV, space\n",
    "\n",
    "def optimize(model, param_grid):\n",
    "    opt = BayesSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        n_iter=32,\n",
    "        random_state=42,\n",
    "        cv=5\n",
    "    )\n",
    "    opt.fit(X_train, y_train)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_params = {\n",
    "    'model__alpha': space.Real(0, 1, prior='uniform'),\n",
    "}\n",
    "\n",
    "opt_lasso = optimize(lasso, lasso_params)\n",
    "best_lasso = opt_lasso.best_estimator_\n",
    "opt_lasso.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(best_lasso)\n",
    "print(f'Optimized Lasso: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ElasticNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_params = {\n",
    "    'model__alpha': space.Real(0, 1, prior='uniform'),\n",
    "    'model__l1_ratio': space.Real(0, 1, prior='uniform')\n",
    "}\n",
    "\n",
    "opt_en = optimize(en, en_params)\n",
    "best_en = opt_en.best_estimator_\n",
    "opt_en.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(best_en)\n",
    "print(f'Optimized ElasticNet: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'colsample_bytree': space.Real(0.1, 0.5, prior='uniform'),\n",
    "    'gamma': space.Real(0.01, 0.1, prior='uniform'),\n",
    "    'learning_rate': space.Real(0.01, 0.1, prior='uniform'),\n",
    "    'max_depth': space.Integer(1, 4),\n",
    "    'min_child_weight': space.Real(1, 2, prior='uniform'),\n",
    "    'n_estimators': space.Integer(100, 2500),\n",
    "    'reg_alpha': space.Real(0.1, 1, prior='uniform'),\n",
    "    'reg_lambda': space.Real(0.1, 1, prior='uniform'),\n",
    "    'subsample': space.Real(0.1, 1, prior='uniform'),\n",
    "}\n",
    "\n",
    "opt_xgb = optimize(xgb, xgb_params)\n",
    "best_xgb = opt_xgb.best_estimator_\n",
    "opt_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(best_xgb)\n",
    "print(f'Optimized XGBoost: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel Ridge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr_params = {\n",
    "    'alpha': space.Real(0, 1, prior='uniform'),\n",
    "    'kernel': space.Categorical(['linear', 'polynomial']),\n",
    "    'degree': space.Integer(2, 5),\n",
    "    'coef0': space.Real(2, 3, prior='uniform')\n",
    "}\n",
    "\n",
    "opt_kr = optimize(kr, kr_params)\n",
    "best_kr = opt_kr.best_estimator_\n",
    "opt_kr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(best_kr)\n",
    "print(f'Optimized Kernel Ridge: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_params = {\n",
    "    'n_estimators': space.Integer(2500, 3000),\n",
    "    'learning_rate': space.Real(0.01, 0.1, prior='uniform'),\n",
    "    'max_depth': space.Integer(1, 5),\n",
    "    'max_features': space.Categorical(['sqrt', 'log2']),\n",
    "    'min_samples_leaf': space.Integer(10, 15),\n",
    "    'min_samples_split': space.Integer(5, 10)\n",
    "}\n",
    "\n",
    "# opt_gb = optimize(gb, gb_params)\n",
    "# best_gb = opt_gb.best_estimator_\n",
    "# opt_gb.best_params_\n",
    "\n",
    "best_gb = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(best_gb)\n",
    "print(f'Optimized Gradient Boosting: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'num_leaves': space.Integer(2, 6),\n",
    "    'learning_rate': space.Real(0.01, 0.1, prior='uniform'), \n",
    "    'n_estimators': space.Integer(100, 1000),\n",
    "    'max_bin': space.Integer(50, 80),\n",
    "    'subsample': space.Real(0.5, 1, prior='uniform'),\n",
    "    'subsample_freq': space.Integer(2, 5),\n",
    "    'colsample_bytree': space.Real(0.1, 0.3, prior='uniform'),\n",
    "    'min_child_samples':space.Integer(2, 6),\n",
    "    'min_child_weight': space.Integer(9, 12)\n",
    "}\n",
    "\n",
    "opt_lgb = optimize(lgb, lgb_params)\n",
    "best_lgb = opt_lgb.best_estimator_\n",
    "opt_lgb.best_params_                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(best_lgb)\n",
    "print(f'Optimized LightGB: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stacking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average base models class** (simplest approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    \n",
    "    # we define clones of the models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "\n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_models = AveragingModels(models=[best_lasso, best_en, best_xgb, best_kr, best_gb, best_lgb])\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(f'Averaged models: {score.mean():.4f}  ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the simplest approach improves the score. This encourages exploring a more complex stacking method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_models.fit(X_train, y_train)\n",
    "\n",
    "# take power of (e+1) to undo the log(x+1) transformation and revert data back to the true house prices\n",
    "y_pred = np.expm1(averaged_models.predict(X_test))\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result['Id'] = test_ID\n",
    "result['SalePrice'] = y_pred\n",
    "result.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "741656c466d97e6cc8d57289f00cb22b14e49f976d4b6ab783fc4f7e7dcfa85a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
